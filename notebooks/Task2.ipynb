{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre",
   "id": "87794d7f82ec4aa3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-24T20:37:54.139024Z",
     "start_time": "2024-10-24T20:37:53.748243Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label\n",
    "\n",
    "from scripts.generate_regularization_layer_test_outputs import test_data\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for num_epochs epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    ax_1.set_ylabel('Error')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    ax_2.set_xlabel('Accuracy')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T20:37:54.604376Z",
     "start_time": "2024-10-24T20:37:54.217772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "# sys.path.append('/path/to/mlpractical')\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ],
   "id": "e6953312e945de27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile '/Users/ycy/PycharmProjects/mlpractical/data/emnist-train.npz' with keys: inputs, targets)\n",
      "KeysView(NpzFile '/Users/ycy/PycharmProjects/mlpractical/data/emnist-valid.npz' with keys: inputs, targets)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline",
   "id": "bf76a334c91d4d70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Setup hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "stats_interval = 1  # Every 1 epoch record the error and acc data\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create model with 3 hidden layer\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ],
   "id": "cb8d284168c4d787"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dropout",
   "id": "44b7bdf17697eb14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T18:30:17.352339Z",
     "start_time": "2024-10-24T18:29:46.312963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Setup hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "stats_interval = 1  # Every 1 epoch record the error and acc data\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "dropout_rate = 0.7\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "stats_list = []\n",
    "keys_list = []\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create model with 3 hidden layer\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    DropoutLayer(rng=rng, incl_prob=dropout_rate),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    DropoutLayer(rng=rng, incl_prob=dropout_rate),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    DropoutLayer(rng=rng, incl_prob=dropout_rate),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ],
   "id": "b410d071aacbcd02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: tqdm in /Users/ycy/opt/anaconda3/envs/mlp/lib/python3.12/site-packages (4.66.5)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ae9d7f9ab794373ba89ad3011034e50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9696eb3c120e45e1b34a714952b4b95e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 8.3s to complete\n",
      "    error(train)=1.32e+00, acc(train)=6.29e-01, error(valid)=1.33e+00, acc(valid)=6.24e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6feca79ce274286bcf491a3575804cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 5.9s to complete\n",
      "    error(train)=1.07e+00, acc(train)=6.91e-01, error(valid)=1.08e+00, acc(valid)=6.87e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcf795f9b65f453387513d1b9d5adb01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 7.8s to complete\n",
      "    error(train)=9.28e-01, acc(train)=7.26e-01, error(valid)=9.46e-01, acc(valid)=7.18e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    final error(train) = 9.28e-01\n",
      "    final error(valid) = 9.46e-01\n",
      "    final acc(train)   = 7.26e-01\n",
      "    final acc(valid)   = 7.18e-01\n",
      "    run time per epoch = 10.01\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stats_dropout = stats\n",
    "keys_dropout = keys"
   ],
   "id": "e21e79577f8f9124"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## L1",
   "id": "95ebad61ddc98437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Setup hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "stats_interval = 1  # Every 1 epoch record the error and acc data\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "dropout_rate = 0.7\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "stats_list = []\n",
    "keys_list = []\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create model with 3 hidden layer\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ],
   "id": "bda4e9717ebb4616"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## L2",
   "id": "cfc84158ba77b0d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T21:38:38.547682Z",
     "start_time": "2024-10-24T21:38:23.147514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlp.penalties import L2Penalty\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Setup hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "stats_interval = 1  # Every 1 epoch record the error and acc data\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "l2_coefficient = 1e-3\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "stats_list = []\n",
    "keys_list = []\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "l2_penalty = L2Penalty(coefficient=l2_coefficient)\n",
    "\n",
    "# Create model with 3 hidden layer\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=l2_penalty, biases_penalty=l2_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=l2_penalty, biases_penalty=l2_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=l2_penalty, biases_penalty=l2_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=l2_penalty, biases_penalty=l2_penalty)\n",
    "])\n",
    "\n",
    "# 原始的交叉熵损失函数\n",
    "error = CrossEntropySoftmaxError()\n",
    "\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "stats_L2 = stats\n",
    "keys_L2 = keys"
   ],
   "id": "6c8588b462129f8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df34df956064428c93ae3995e90f75ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11ea74d17d614e87b7ba504dbc83c992"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 7.0s to complete\n",
      "    error(train)=3.85e+00, acc(train)=2.12e-02, error(valid)=3.85e+00, acc(valid)=2.15e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "349c0dbe5c2f480aa4ab5e017ec33231"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m learning_rule \u001B[38;5;241m=\u001B[39m AdamLearningRule(learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Remember to use notebook=False when you write a script to be run in a terminal\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 \u001B[38;5;241m=\u001B[39m train_model_and_plot_stats(\n\u001B[1;32m     44\u001B[0m     model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m    final error(train) = \u001B[39m\u001B[38;5;132;01m{0:.2e}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(stats[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keys[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror(train)\u001B[39m\u001B[38;5;124m'\u001B[39m]]))\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m    final error(valid) = \u001B[39m\u001B[38;5;132;01m{0:.2e}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(stats[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keys[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror(valid)\u001B[39m\u001B[38;5;124m'\u001B[39m]]))\n",
      "Cell \u001B[0;32mIn[69], line 19\u001B[0m, in \u001B[0;36mtrain_model_and_plot_stats\u001B[0;34m(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook)\u001B[0m\n\u001B[1;32m     14\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m Optimiser(\n\u001B[1;32m     15\u001B[0m     model, error, learning_rule, train_data, valid_data, data_monitors, notebook\u001B[38;5;241m=\u001B[39mnotebook)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Run the optimiser for num_epochs epochs (full passes through the training set)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# printing statistics every epoch.\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m stats, keys, run_time \u001B[38;5;241m=\u001B[39m optimiser\u001B[38;5;241m.\u001B[39mtrain(num_epochs\u001B[38;5;241m=\u001B[39mnum_epochs, stats_interval\u001B[38;5;241m=\u001B[39mstats_interval)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Plot the change in the validation and training set error over training.\u001B[39;00m\n\u001B[1;32m     22\u001B[0m fig_1 \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n",
      "File \u001B[0;32m~/PycharmProjects/mlpractical/mlp/optimisers.py:138\u001B[0m, in \u001B[0;36mOptimiser.train\u001B[0;34m(self, num_epochs, stats_interval)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    137\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_training_epoch()\n\u001B[1;32m    139\u001B[0m     epoch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m stats_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/mlpractical/mlp/optimisers.py:67\u001B[0m, in \u001B[0;36mOptimiser.do_training_epoch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     64\u001B[0m grads_wrt_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mgrad(activations[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], targets_batch)\n\u001B[1;32m     65\u001B[0m grads_wrt_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgrads_wrt_params(\n\u001B[1;32m     66\u001B[0m     activations, grads_wrt_outputs)\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rule\u001B[38;5;241m.\u001B[39mupdate_params(grads_wrt_params)\n\u001B[1;32m     68\u001B[0m train_progress_bar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/mlpractical/mlp/learning_rules.py:254\u001B[0m, in \u001B[0;36mAdamLearningRule.update_params\u001B[0;34m(self, grads_wrt_params)\u001B[0m\n\u001B[1;32m    248\u001B[0m     mom_2 \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_2) \u001B[38;5;241m*\u001B[39m grad \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    249\u001B[0m     alpha_t \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    250\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    251\u001B[0m             (\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_2 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m/\u001B[39m\n\u001B[1;32m    252\u001B[0m             (\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_1 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    253\u001B[0m     )\n\u001B[0;32m--> 254\u001B[0m     param \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m alpha_t \u001B[38;5;241m*\u001B[39m mom_1 \u001B[38;5;241m/\u001B[39m (mom_2 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon)\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T20:37:43.885604Z",
     "start_time": "2024-10-24T20:37:43.772434Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats_dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     38\u001B[0m     fig_1\u001B[38;5;241m.\u001B[39msavefig(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../outputs/task2-Dropout-error.pdf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 40\u001B[0m plot_training_stats_layers(stats_list, keys_list, stats_interval)\n",
      "Cell \u001B[0;32mIn[68], line 15\u001B[0m, in \u001B[0;36mplot_training_stats_layers\u001B[0;34m(stats_list, keys_list, stats_interval)\u001B[0m\n\u001B[1;32m     12\u001B[0m ax_2 \u001B[38;5;241m=\u001B[39m ax_1\u001B[38;5;241m.\u001B[39mtwinx()\n\u001B[1;32m     13\u001B[0m ax_2\u001B[38;5;241m.\u001B[39mset_ylabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGeneralization gap\u001B[39m\u001B[38;5;124m'\u001B[39m, rotation\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m90\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m error_train \u001B[38;5;241m=\u001B[39m stats_dropout[\u001B[38;5;241m1\u001B[39m:, keys_dropout[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror(train)\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     16\u001B[0m error_valid \u001B[38;5;241m=\u001B[39m stats_dropout[\u001B[38;5;241m1\u001B[39m:, keys_dropout[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror(valid)\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     17\u001B[0m generalization_gap \u001B[38;5;241m=\u001B[39m error_valid \u001B[38;5;241m-\u001B[39m error_train\n",
      "\u001B[0;31mNameError\u001B[0m: name 'stats_dropout' is not defined"
     ]
    }
   ],
   "execution_count": 68,
   "source": [
    "# wrong  but in Coursework is right Please compare and learn from it.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "error_train_dropout = stats_dropout[1:, keys_dropout['error(train)']]\n",
    "error_valid_dropout = stats_dropout[1:, keys_dropout['error(valid)']]\n",
    "generalization_gap_dropout = error_valid_dropout - error_train_dropout\n",
    "acc_valid_dropout = stats_dropout[1:, keys_dropout['acc(valid)']]\n",
    "\n",
    "error_train_l1 = stats_l1[1:, keys_l1['error(train)']]\n",
    "error_valid_l1 = stats_l1[1:, keys_l1['error(valid)']]\n",
    "generalization_gap_l1 = error_valid_l1 - error_train_l1\n",
    "acc_valid_l1 = stats_l1[1:, keys_l1['acc(valid)']]\n",
    "\n",
    "error_train_l2 = stats_l2[1:, keys_l2['error(train)']]\n",
    "error_valid_l2 = stats_l2[1:, keys_l2['error(valid)']]\n",
    "generalization_gap_l2 = error_valid_l2 - error_train_l2\n",
    "acc_valid_l2 = stats_l2[1:, keys_l2['acc(valid)']]\n",
    "\n",
    "def plot_training_stats_regularization():\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    ax_1 = fig_1.add_subplot(121)\n",
    "    ax_1.set_xlabel('Dropout value')\n",
    "    ax_1.set_ylabel('Accuracy')\n",
    "    ax_2 = ax_1.twinx()\n",
    "    ax_2.set_ylabel('Generalization gap', rotation=90)\n",
    "    \n",
    "    ax_1.plot(np.arange(1, len(acc_valid_dropout) + 1), acc_valid_dropout, label='Val. Acc', color='r')\n",
    "    ax_2.plot(np.arange(1, len(generalization_gap_dropout) + 1), generalization_gap_dropout, label='Gap', color='b')\n",
    "\n",
    "    ax_1.legend(loc='upper left')\n",
    "    ax_2.legend(loc='upper right')\n",
    "    \n",
    "\n",
    "    ax_3 = fig_1.add_subplot(122)\n",
    "    ax_3.set_xlabel('Weight dacay value')\n",
    "    ax_3.set_ylabel('Accuracy', rotation=90)\n",
    "    ax_4 = ax_3.twinx()\n",
    "    ax_4.set_ylabel('Generalization gap', rotation=90)\n",
    "    \n",
    "    ax_3.plot(np.arange(1, len(acc_valid_l1) + 1), acc_valid_l1, label='L1 Val. Acc', color='r')\n",
    "    ax_3.plot(np.arange(1, len(generalization_gap_dropout) + 1), generalization_gap_dropout, label='L1 Gap', color='b')\n",
    "    ax_4.plot(np.arange(1, len(acc_valid_l2) + 1), acc_valid_l2, label='L1 Val. Acc', color='r')\n",
    "    ax_4.plot(np.arange(1, len(generalization_gap_dropout) + 1), generalization_gap_dropout, label='L1 Gap', color='b')\n",
    "    \n",
    "    plt.show()\n",
    "    fig_1.savefig('../outputs/task2-regularization.pdf')\n",
    "\n",
    "plot_training_stats_regularization()\n"
   ],
   "id": "f2706ebe95667105"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## label smoothing",
   "id": "a2991703d6a76a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng, smooth_labels=True)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Setup hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "stats_interval = 1  # Every 1 epoch record the error and acc data\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create model with 3 hidden layer\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ],
   "id": "d96e81e6f9146259"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
